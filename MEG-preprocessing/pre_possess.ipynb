{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, output_file):\n",
    "    with open(output_file, 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fif_file = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed/preprocessed_P1-epo.fif\"\n",
    "output_dir = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed_npy\"\n",
    "def read_and_crop_epochs(fif_file):\n",
    "    epochs = mne.read_epochs(fif_file, preload=True)\n",
    "    cropped_epochs = epochs.crop(tmin=0, tmax=1.0)\n",
    "    return cropped_epochs\n",
    "\n",
    "epochs = read_and_crop_epochs(fif_file)    \n",
    "sorted_indices = np.argsort(epochs.events[:, 2])\n",
    "epochs = epochs[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(epochs.events))\n",
    "epochs.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_file_path = '/home/ldy/Workspace/THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv'\n",
    "image_concept_df = pd.read_csv(csv_file_path, header=None)\n",
    "print(image_concept_df)\n",
    "\n",
    "\n",
    "# Accessing a column by its name\n",
    "# Display the first few rows to understand its structure\n",
    "image_concept_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_valid_epochs(epochs, exclude_event_id=999999):\n",
    "    return epochs[epochs.events[:, 2] != exclude_event_id]\n",
    "\n",
    "valid_epochs = filter_valid_epochs(epochs)\n",
    "valid_epochs.info\n",
    "valid_epochs.events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_zs_event_ids(epochs, num_repetitions=12):\n",
    "    event_ids = epochs.events[:, 2]\n",
    "    unique_event_ids, counts = np.unique(event_ids, return_counts=True)\n",
    "    zs_event_ids = unique_event_ids[counts == num_repetitions]\n",
    "    return zs_event_ids\n",
    "\n",
    "zs_event_ids = identify_zs_event_ids(valid_epochs)\n",
    "# Verify the zero-shot event IDs\n",
    "print(\"Zero-shot Event IDs:\", zs_event_ids)\n",
    "\n",
    "len(zs_event_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate and process datasets\n",
    "training_epochs = valid_epochs[~np.isin(valid_epochs.events[:, 2], zs_event_ids)]\n",
    "# Verify the number of events in the training set\n",
    "print(\"Number of events in the training set:\", len(training_epochs.events))\n",
    "print(len(training_epochs.events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract event IDs from the filtered training epochs\n",
    "training_event_ids = np.unique(training_epochs.events[:, 2])\n",
    "\n",
    "# Check for any overlap between zero-shot and training event IDs\n",
    "overlap_ids = np.intersect1d(zs_event_ids, training_event_ids)\n",
    "\n",
    "# Print the overlap, if any\n",
    "print(\"Overlapping Event IDs:\", overlap_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_test_epochs = valid_epochs[np.isin(valid_epochs.events[:, 2], zs_event_ids)]\n",
    "zs_test_epochs.events\n",
    "len(zs_test_epochs.events)\n",
    "# zs_test_epochs.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_epochs.events))\n",
    "print(len(zs_test_epochs.events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs.events[:, -1]\n",
    "zs_test_epochs.events[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_event_ids = training_epochs.events[:, -1]\n",
    "test_event_ids = zs_test_epochs.events[:, -1]\n",
    "\n",
    "counts = {test_id: np.sum(training_event_ids == test_id) for test_id in test_event_ids}\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming zs_event_ids is a numpy array or a list of event IDs\n",
    "# Assuming image_concept_df is a pandas DataFrame with one column '1' representing image category indices\n",
    "\n",
    "zs_event_to_category_map = {}\n",
    "\n",
    "for i, event_id in enumerate(zs_event_ids):\n",
    "    # Using the row index (i) to map to the image category index\n",
    "    # Assuming the first event_id corresponds to the first row, second event_id to the second row, and so on\n",
    "    image_category_index = image_concept_df.iloc[event_id-1, 0]  # Accessing the first (and only) column at row i\n",
    "    zs_event_to_category_map[event_id] = image_category_index\n",
    "\n",
    "# Print the mapping\n",
    "print(\"Event ID to Image Category Index Mapping:\")\n",
    "for event_id, category_index in zs_event_to_category_map.items():\n",
    "    print(f\"Event ID {event_id}: Image Category Index {category_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all the categories in the test set\n",
    "test_set_categories = []\n",
    "\n",
    "# Iterate over the event IDs in the test set\n",
    "for event_id in zs_event_ids:\n",
    "    if event_id in zs_event_to_category_map:\n",
    "        # Get the category index from the mapping\n",
    "        category_index = zs_event_to_category_map[event_id]\n",
    "        test_set_categories.append(category_index)\n",
    "\n",
    "# Print the list of categories in the test set\n",
    "print(\"Categories in the test set:\", test_set_categories)\n",
    "len(test_set_categories)\n",
    "# test_set_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each category ID in the training set\n",
    "category_counts = Counter(test_set_categories)\n",
    "\n",
    "# Print the counts of each category ID\n",
    "print(\"Counts of each category ID in the training set:\")\n",
    "for category_id, count in category_counts.items():\n",
    "    print(f\"Category ID {category_id}: Count {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming zs_event_ids is a numpy array or a list of event IDs\n",
    "# Assuming image_concept_df is a pandas DataFrame with one column '1' representing image category indices\n",
    "\n",
    "event_to_category_map = {}\n",
    "\n",
    "for i, event_id in enumerate(training_event_ids):\n",
    "    # Using the row index (i) to map to the image category index\n",
    "    # Assuming the first event_id corresponds to the first row, second event_id to the second row, and so on\n",
    "    image_category_index = image_concept_df.iloc[event_id-1, 0]  # Accessing the first (and only) column at row i\n",
    "    event_to_category_map[event_id] = image_category_index\n",
    "\n",
    "# Print the mapping\n",
    "print(\"Event ID to Image Category Index Mapping:\")\n",
    "for event_id, category_index in event_to_category_map.items():\n",
    "    print(f\"Event ID {event_id}: Image Category Index {category_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming training_epochs is a variable that contains your training set epochs\n",
    "# And it has an 'events' attribute similar to zs_test_epochs\n",
    "\n",
    "# List to hold all the categories in the training set\n",
    "train_set_categories = []\n",
    "\n",
    "# Extract event IDs from the training set\n",
    "training_event_ids = training_epochs.events[:, 2]\n",
    "\n",
    "# Iterate over the event IDs in the training set\n",
    "for event_id in training_event_ids:\n",
    "    if event_id in event_to_category_map:\n",
    "        # Get the category index from the mapping\n",
    "        category_index = event_to_category_map[event_id]        \n",
    "        train_set_categories.append(category_index)\n",
    "\n",
    "# Print the list of categories in the training set\n",
    "print(\"Categories in the training set:\", train_set_categories)\n",
    "print(\"Total number of category entries in the training set:\", len(train_set_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the occurrences of each category ID in the training set\n",
    "category_counts = Counter(train_set_categories)\n",
    "\n",
    "# Print the counts of each category ID\n",
    "print(\"Counts of each category ID in the training set:\")\n",
    "for category_id, count in category_counts.items():\n",
    "    print(f\"Category ID {category_id}: Count {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {test_id: np.sum(train_set_categories == test_id) for test_id in test_set_categories}\n",
    "# Calculate the total number of elements in 'counts'\n",
    "total_elements = sum(counts.values())\n",
    "\n",
    "# Print the total number of elements\n",
    "print(\"Total number of elements represented in 'counts':\", total_elements)\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_set_categories and test_set_categories are lists or numpy arrays\n",
    "\n",
    "# Create a new list with elements from train_set_categories that are not in test_set_categories\n",
    "train_set_categories_filtered = [item for item in train_set_categories if item not in test_set_categories]\n",
    "\n",
    "# train_set_categories_filtered now contains elements from train_set_categories excluding those in test_set_categories\n",
    "print(\"Filtered train_set_categories:\", train_set_categories_filtered)\n",
    "len(train_set_categories_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask for epochs to keep in the training set\n",
    "keep_epochs_mask = [category not in test_set_categories for category in train_set_categories]\n",
    "keep_epochs_mask\n",
    "# Apply the mask to filter out epochs from training_epochs\n",
    "training_epochs_filtered = training_epochs[keep_epochs_mask]\n",
    "\n",
    "# Confirm the filtering\n",
    "print(\"Original training set size:\", len(training_epochs))\n",
    "print(\"Filtered training set size:\", len(training_epochs_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_meg_data(epochs, num_concepts, num_imgs, repetitions):\n",
    "    data = epochs.get_data()\n",
    "    reshaped_data = data.reshape((num_concepts, num_imgs, repetitions, data.shape[1], data.shape[2]))\n",
    "    return reshaped_data\n",
    "\n",
    "\n",
    "training_data = reshape_meg_data(training_epochs_filtered, num_concepts=1654, num_imgs=12, repetitions=1)\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_test_data = reshape_meg_data(zs_test_epochs, num_concepts=200, num_imgs=1, repetitions=12)\n",
    "zs_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data\n",
    "# if not os.path.isdir(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "# save_data({'meg_data': training_data, 'ch_names': training_epochs_filtered.ch_names, 'times': training_epochs_filtered.times},\n",
    "#             os.path.join(output_dir, 'preprocessed_meg_training.pkl'))\n",
    "# save_data({'meg_data': zs_test_data, 'ch_names': zs_test_epochs.ch_names, 'times': zs_test_epochs.times},\n",
    "#             os.path.join(output_dir, 'preprocessed_meg_zs_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_and_save_meg_data(fif_file, output_dir):\n",
    "    epochs = read_and_crop_epochs(fif_file)\n",
    "    \n",
    "    sorted_indices = np.argsort(epochs.events[:, 2])\n",
    "    epochs = epochs[sorted_indices]\n",
    "\n",
    "    valid_epochs = filter_valid_epochs(epochs)\n",
    "    zs_event_ids = identify_zs_event_ids(valid_epochs)\n",
    "\n",
    "    training_epochs = valid_epochs[~np.isin(valid_epochs.events[:, 2], zs_event_ids)]\n",
    "    zs_test_epochs = valid_epochs[np.isin(valid_epochs.events[:, 2], zs_event_ids)]\n",
    "\n",
    "    keep_epochs_mask = [category not in test_set_categories for category in train_set_categories]\n",
    "    training_epochs_filtered = training_epochs[keep_epochs_mask]\n",
    "\n",
    "    training_data = reshape_meg_data(training_epochs_filtered, num_concepts=1654, num_imgs=12, repetitions=1)\n",
    "    zs_test_data = reshape_meg_data(zs_test_epochs, num_concepts=200, num_imgs=1, repetitions=12)\n",
    "\n",
    "    # Save data\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    save_data({'meg_data': training_data, 'ch_names': training_epochs_filtered.ch_names, 'times': training_epochs_filtered.times},\n",
    "              os.path.join(output_dir, 'preprocessed_meg_training.pkl'))\n",
    "    save_data({'meg_data': zs_test_data, 'ch_names': zs_test_epochs.ch_names, 'times': zs_test_epochs.times},\n",
    "              os.path.join(output_dir, 'preprocessed_meg_zs_test.pkl'))\n",
    "\n",
    "# fif_file = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed/preprocessed_P1-epo.fif\"\n",
    "# output_dir = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed_npy\"\n",
    "# process_and_save_meg_data(fif_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_save_meg_data(fif_file, output_dir):\n",
    "#     epochs = read_and_crop_epochs(fif_file)    \n",
    "#     valid_epochs = filter_valid_epochs(epochs)    \n",
    "#     zs_event_ids = identify_zs_event_ids(valid_epochs)\n",
    "    \n",
    "#     training_epochs = valid_epochs[~np.isin(valid_epochs.events[:, 2], zs_event_ids)]    \n",
    "#     zs_test_epochs = valid_epochs[np.isin(valid_epochs.events[:, 2], zs_event_ids)]\n",
    "    \n",
    "#     keep_epochs_mask = [category not in test_set_categories for category in train_set_categories]    \n",
    "#     training_epochs_filtered = training_epochs[keep_epochs_mask]\n",
    "\n",
    "\n",
    "#     training_data = reshape_meg_data(training_epochs_filtered, num_concepts=1654, num_imgs=12, repetitions=1)\n",
    "#     zs_test_data = reshape_meg_data(zs_test_epochs, num_concepts=200, num_imgs=1, repetitions=12)\n",
    "\n",
    "#     # Save data\n",
    "#     if not os.path.isdir(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "#     save_data({'meg_data': training_data, 'ch_names': training_epochs_filtered.ch_names, 'times': training_epochs_filtered.times},\n",
    "#                 os.path.join(output_dir, 'preprocessed_meg_training.pkl'))\n",
    "#     save_data({'meg_data': zs_test_data, 'ch_names': zs_test_epochs.ch_names, 'times': zs_test_epochs.times},\n",
    "#                 os.path.join(output_dir, 'preprocessed_meg_zs_test.pkl'))\n",
    "\n",
    "# # fif_file = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed/preprocessed_P1-epo.fif\"\n",
    "# # output_dir = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed_npy\"\n",
    "# # process_and_save_meg_data(fif_file, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, output_dir):\n",
    "    fif_files = glob.glob(os.path.join(input_dir, '**/*epo.fif'), recursive=True)\n",
    "    for fif_file in fif_files:\n",
    "        filename = os.path.basename(fif_file)\n",
    "        subject_num = filename.split('_')[1].split('-')[0]\n",
    "        subject_dir_name = f\"sub-{int(subject_num[1:]):02d}\"\n",
    "        subject_output_dir = os.path.join(output_dir, subject_dir_name)\n",
    "        process_and_save_meg_data(fif_file, subject_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed/\"\n",
    "output_dir = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed_npy\"\n",
    "process_directory(in_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy  as np\n",
    "csv_img_file_path = \"/home/ldy/Workspace/THINGS/osfstorage/THINGS/Metadata/Image-specific/image_paths.csv\"\n",
    "origin_img_dir = \"/home/ldy/Workspace/THINGS/osfstorage/THINGS/Images/\"\n",
    "training_images_dir = \"/home/ldy/Workspace/THINGS/osfstorage/THINGS/images_set/training_images\"\n",
    "test_images_dir = \"/home/ldy/Workspace/THINGS/osfstorage/THINGS/images_set/test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.read_csv(csv_img_file_path, header=None)\n",
    "print(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs_filtered.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "concept_csv_file_path = '/home/ldy/Workspace/THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv'\n",
    "image_concept_df = pd.read_csv(concept_csv_file_path, header=None)\n",
    "print(image_concept_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_epochs_filtered.events[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "concept_csv_file_path = '/home/ldy/Workspace/THINGS/osfstorage/THINGS/Metadata/Concept-specific/image_concept_index.csv'\n",
    "image_concept_df = pd.read_csv(concept_csv_file_path, header=None)\n",
    "\n",
    "for index, row in image_df.iterrows():\n",
    "    source_image_path = row[0]\n",
    "\n",
    "    path_parts = source_image_path.split('/')\n",
    "    if len(path_parts) > 2:\n",
    "        formatted_index = str(category_index).zfill(5)\n",
    "        path_parts[1] = f\"{formatted_index}_{path_parts[1]}\"\n",
    "    image_path = '/'.join(path_parts)\n",
    "    \n",
    "    if event_id in training_epochs_filtered.events[:, -1]:\n",
    "        target_dir = os.path.join(training_images_dir)\n",
    "    elif event_id in zs_test_epochs.events[:, -1]:\n",
    "        target_dir = os.path.join(test_images_dir)\n",
    "    else:\n",
    "        continue\n",
    "    # print(image_path)\n",
    "    src_file = os.path.join(origin_img_dir, source_image_path)\n",
    "    dest_file = os.path.join(target_dir, image_path)\n",
    "    # print(src_file)\n",
    "    # print(target_dir)\n",
    "    # print(dest_file)\n",
    "    os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "    shutil.copy(src_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in image_df.iterrows():\n",
    "#     image_path = row[0]\n",
    "\n",
    "#     if event_id in training_epochs_filtered.events[:, -1]:\n",
    "#         target_dir = training_images_dir\n",
    "#     elif event_id in zs_test_epochs.events[:, -1]:\n",
    "#         target_dir = test_images_dir\n",
    "#     else:\n",
    "\n",
    "#     src_file = os.path.join(origin_img_dir, image_path)\n",
    "#     dest_file = os.path.join(target_dir, image_path)\n",
    "\n",
    "#     os.makedirs(os.path.dirname(dest_file), exist_ok=True)\n",
    "#     shutil.copy(src_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_images_dir = \"/home/ldy/Workspace/THINGS/osfstorage/THINGS/images_set/training_images/images/\"\n",
    "\n",
    "def count_images(directory):\n",
    "    total_dirs = 0\n",
    "    total_images = 0\n",
    "\n",
    "    for entry in os.listdir(directory):\n",
    "        path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(path):\n",
    "            total_dirs += 1\n",
    "            total_images += len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
    "\n",
    "    return total_dirs, total_images\n",
    "\n",
    "num_dirs, num_images = count_images(training_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed_npy/sub-01/preprocessed_meg_training.pkl\"\n",
    "\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "meg_data = data['meg_data']\n",
    "ch_names = data['ch_names']\n",
    "times = data['times']\n",
    "meg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ldy/Workspace/THINGS_MEG/ds004212/derivatives/preprocessed_npy/sub-01/preprocessed_meg_zs_test.pkl\"\n",
    "with open(path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "meg_data = data['meg_data']\n",
    "ch_names = data['ch_names']\n",
    "times = data['times']\n",
    "meg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(meg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming meg_data is of shape (samples, channels, time_points)\n",
    "first_sample = meg_data[0, 0, 0, :, :]  # Select the first sample\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 8))  # Adjust the size as needed\n",
    "\n",
    "# You may not want to plot all channels if there are many, so adjust this as needed\n",
    "for i, ch_name in enumerate(ch_names):\n",
    "    plt.plot(times, first_sample[i, :], label=ch_name)\n",
    "\n",
    "plt.xlabel('Time (s)')  # Assuming 'times' is in seconds\n",
    "plt.ylabel('MEG Signal')  # Adjust label as appropriate\n",
    "plt.title('MEG Waveform of the First Sample')\n",
    "plt.legend()  # Comment out if there are too many channels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
