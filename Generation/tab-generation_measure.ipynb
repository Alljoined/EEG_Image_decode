{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from eegdatasets_leaveone_measure import EEGDataset\n",
    "from eegencoder import eeg_encoder\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from lavis.models.clip_models.loss import ClipLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from utils import wandb_logger\n",
    "from braindecode.models import EEGNetv4, ATCNet, EEGConformer, EEGITNet, ShallowFBCSPNet\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model + 1, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[:d_model // 2 + 1])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:d_model // 2])\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:x.size(0), :].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EEGAttention(nn.Module):\n",
    "    def __init__(self, channel, d_model, nhead):\n",
    "        super(EEGAttention, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.channel = channel\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(2, 0, 1)  # Change shape to [time_length, batch_size, channel]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output.permute(1, 2, 0)  # Change shape back to [batch_size, channel, time_length]\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "        # revised from shallownet\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.AvgPool2d((1, 51), (1, 5)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(40, 40, (63, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  \n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b, _, _, _ = x.shape\n",
    "        x = x.unsqueeze(1)     \n",
    "        # print(\"x\", x.shape)   \n",
    "        x = self.tsconv(x)\n",
    "        # print(\"tsconv\", x.shape)   \n",
    "        x = self.projection(x)\n",
    "        # print(\"projection\", x.shape)  \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(emb_size),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "        \n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1440, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class Proj_img(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1024, proj_dim=1024, drop_proj=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "class ATM_S(nn.Module):    \n",
    "    def __init__(self, num_channels=63, sequence_length=250, num_subjects=1, num_features=64, num_latents=1024, num_blocks=1):\n",
    "        super(ATM_S, self).__init__()\n",
    "        self.attention_model = EEGAttention(num_channels, num_channels, nhead=1)   \n",
    "        self.subject_wise_linear = nn.ModuleList([nn.Linear(sequence_length, sequence_length) for _ in range(num_subjects)])\n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg()        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_func = ClipLoss()       \n",
    "         \n",
    "    def forward(self, x):\n",
    "        x = self.attention_model(x)\n",
    "        # print(f'After attention shape: {x.shape}')\n",
    "         \n",
    "        x = self.subject_wise_linear[0](x)\n",
    "        # print(f'After subject-specific linear transformation shape: {x.shape}')\n",
    "        eeg_embedding = self.enc_eeg(x)\n",
    "        \n",
    "        out = self.proj_eeg(eeg_embedding)\n",
    "        return out  \n",
    "    \n",
    "      \n",
    "\n",
    "def train_model(eegmodel, imgmodel, dataloader, optimizer, device, text_features_all, img_features_all):\n",
    "    eegmodel.train()\n",
    "    text_features_all = text_features_all.to(device).float() # (n_cls, d)\n",
    "    img_features_all = (img_features_all[::10]).to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha=0.9\n",
    "    features_list = []  # List to store features\n",
    "    save_features= True\n",
    "    ridge_lambda = 0.1\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "        eeg_data = eeg_data.to(device)\n",
    "        text_features = text_features.to(device).float()\n",
    "        img_features = img_features.to(device).float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        eeg_features = eegmodel(eeg_data).float()\n",
    "        # img_features_outputs = regression(eeg_features).float()\n",
    "        features_list.append(eeg_features)\n",
    "        logit_scale = eegmodel.logit_scale\n",
    "        img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "        text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "        contrastive_loss = img_loss\n",
    "        # print(\"text_loss\", text_loss)\n",
    "        # print(\"img_loss\", img_loss)\n",
    "        \n",
    "        regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "        # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "        loss = (alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # logits = logit_scale * eeg_features @ text_features_all.T # (n_batch, n_cls)\n",
    "        logits_img = logit_scale * eeg_features @ img_features_all.T\n",
    "        # logits_text = logit_scale * eeg_features @ text_features_all.T\n",
    "        # logits_single = (logits_text + logits_img) / 2.0        \n",
    "        # logits_text = logit_scale * eeg_features @ text_features_all.T\n",
    "        logits_single = logits_img\n",
    "        predicted = torch.argmax(logits_single, dim=1) # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "\n",
    "        batch_size = predicted.shape[0]\n",
    "        total += batch_size\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy\n",
    "\n",
    "def evaluate_model(eegmodel, imgmodel, dataloader, device, text_features_all, img_features_all, generated_features_all, k):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.9\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    ridge_lambda = 0.1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            eeg_data = eeg_data.to(device)    \n",
    "            \n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            eeg_features = eegmodel(eeg_data).float()\n",
    "            logit_scale = eegmodel.logit_scale                    \n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"eeg_features\", eeg_features.shape)\n",
    "            # print(torch.std(eeg_features, dim=-1))\n",
    "            # print(torch.std(img_features, dim=-1))\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)            \n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = generated_features_all[::10][selected_classes]\n",
    "                # selected_img_features = img_features_all[selected_classes]\n",
    "                selected_text_features = text_features_all[selected_classes]\n",
    "                \n",
    "                if k==200:\n",
    "                    logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                    logits_single = logits_img\n",
    "                    # print(\"logits_single\", logits_single.shape)\n",
    "                    # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                    predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                    if predicted_label == label.item():\n",
    "                        # print(\"predicted_label\", predicted_label)\n",
    "                        correct += 1\n",
    "                    \n",
    "                    # print(\"logits_single\", logits_single)\n",
    "                    _, top5_indices = torch.topk(logits_single, 5, largest =True)\n",
    "                                                           \n",
    "                    if label.item() in [selected_classes[i] for i in top5_indices.tolist()]:                \n",
    "                        top5_correct_count+=1                                \n",
    "                    total += 1\n",
    "                elif k == 50 or k == 100:\n",
    "                    selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                    selected_img_features = generated_features_all[::10][selected_classes]\n",
    "\n",
    "                    logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                    logits_single = logits_img\n",
    "                    \n",
    "                    predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                    if predicted_label == label.item():\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                elif k==2 or k==4 or k==10:\n",
    "                    selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                    selected_img_features = generated_features_all[::10][selected_classes]\n",
    "                    logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                    # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                    # logits_single = (logits_text + logits_img) / 2.0\n",
    "                    logits_single = logits_img\n",
    "                    # print(\"logits_single\", logits_single.shape)\n",
    "                    # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                    predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                    if predicted_label == label.item():\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                else:\n",
    "                    print(\"Error.\")\n",
    "                    \n",
    "    # print(\"total_loss\", total_loss)\n",
    "    # print(\"batch_idx+1\", batch_idx+1)                \n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    top5_acc = top5_correct_count / total\n",
    "    return average_loss, accuracy, top5_acc\n",
    "\n",
    "def main_train_loop(sub, eeg_model, img_model, train_dataloader, test_dataloader, optimizer, device, \n",
    "                    text_features_train_all, text_features_test_all, img_features_train_all, img_features_test_all, generated_features_all, config, logger=None):\n",
    "    logger = wandb_logger(config) if logger else None\n",
    "    logger.watch(eeg_model,logger) \n",
    "    logger.watch(img_model,logger) \n",
    "    train_losses, train_accuracies = [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    v2_accs = []\n",
    "    v4_accs = []\n",
    "    v10_accs = []\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_model_weights = None\n",
    "    best_epoch_info = {}\n",
    "    results = []  # List to store results for each epoch\n",
    "    current_time = datetime.datetime.now().strftime(\"%m-%d_%H-%M\")  \n",
    "    for epoch in range(config['epochs']):\n",
    "        # train_loss, train_accuracy = train_model(eeg_model, img_model, train_dataloader, optimizer, device, text_features_train_all, img_features_train_all)\n",
    "        # if (epoch +1) % 5 == 0:               \n",
    "        #     if config['insubject']==True:       \n",
    "        #         os.makedirs(f\"./models/contrast/{config['encoder_type']}/{sub}/{current_time}\", exist_ok=True)             \n",
    "        #         file_path = f\"./models/contrast/{config['encoder_type']}/{sub}/{current_time}/{epoch+1}.pth\"\n",
    "        #         torch.save(eeg_model.state_dict(), file_path)            \n",
    "        #     else:                \n",
    "        #         os.makedirs(f\"./models/contrast/across/{config['encoder_type']}/{current_time}\", exist_ok=True)             \n",
    "        #         file_path = f\"./models/contrast/across/{config['encoder_type']}/{current_time}/{epoch+1}.pth\"\n",
    "        #         torch.save(eeg_model.state_dict(), file_path)\n",
    "        #     print(f\"model saved in {file_path}!\")\n",
    "        eeg_model.load_state_dict(torch.load(f\"models/contrast/ATMS/{sub}/40.pth\", map_location=device))\n",
    "        eeg_model = eeg_model.to(device)\n",
    "        generated_features_all = generated_features_all.to(device)\n",
    "        # eegmodel.eval()\n",
    "        test_loss, test_accuracy, top5_acc = evaluate_model(eeg_model, img_model, test_dataloader, device, text_features_test_all, img_features_test_all, generated_features_all, k=200)\n",
    "        _, v2_acc, _ = evaluate_model(eeg_model, img_model, test_dataloader, device, text_features_test_all, img_features_test_all, generated_features_all, k = 2)\n",
    "        _, v4_acc, _ = evaluate_model(eeg_model, img_model, test_dataloader, device, text_features_test_all, img_features_test_all, generated_features_all, k = 4)\n",
    "        _, v10_acc, _ = evaluate_model(eeg_model, img_model, test_dataloader, device, text_features_test_all, img_features_test_all, generated_features_all, k = 10)\n",
    "        _, v50_acc = evaluate_model(eeg_model, img_model, test_dataloader, device, text_features_test_all, img_features_test_all, generated_features_all, k=50)\n",
    "        _, v100_acc = evaluate_model(eeg_model, img_model, test_dataloader, device, text_features_test_all, img_features_test_all, generated_features_all, k=100)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        v2_accs.append(v2_acc)\n",
    "        v4_accs.append(v4_acc)\n",
    "        v10_accs.append(v10_acc)\n",
    "        \n",
    "        # Append results for this epoch\n",
    "        epoch_results = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        # \"train_loss\": train_loss,\n",
    "        # \"train_accuracy\": train_accuracy,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"v2_acc\": v2_acc,\n",
    "        \"v4_acc\": v4_acc,\n",
    "        \"v10_acc\": v10_acc,\n",
    "        \"top5_acc\":top5_acc,\n",
    "        \"v50_acc\": v50_acc,\n",
    "        \"v100_acc\": v100_acc\n",
    "        }\n",
    "\n",
    "        results.append(epoch_results)\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            # best_model_weights = model.state_dict().copy()\n",
    "            \n",
    "            best_epoch_info = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                # \"train_loss\": train_loss,\n",
    "                # \"train_accuracy\": train_accuracy,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_accuracy\": test_accuracy,\n",
    "                \"v2_acc\":v2_acc,\n",
    "                \"v4_acc\":v4_acc,\n",
    "                \"v10_acc\":v10_acc\n",
    "            }\n",
    "        logger.log({\n",
    "            # \"Train Loss\": train_loss,\n",
    "            # \"Train Accuracy\": train_accuracy,\n",
    "            \"Test Loss\": test_loss,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"v2 Accuracy\": v2_acc,\n",
    "            \"v4 Accuracy\": v4_acc,\n",
    "            \"v10 Accuracy\": v10_acc,\n",
    "            \"Epoch\": epoch\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Top5 Accuracy: {top5_acc:.4f}\")\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']} - v2 Accuracy:{v2_acc} - v4 Accuracy:{v4_acc} - v10 Accuracy:{v10_acc} - v50 Accuracy:{v50_acc} - v100 Accuracy:{v100_acc}\")\n",
    "  \n",
    "    # model.load_state_dict(best_model_weights)\n",
    "\n",
    "    # torch.save(model.state_dict(), '{train_pos_img_text}.pth')\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "    axs[0, 0].plot(train_losses, label='Train Loss')\n",
    "    axs[0, 0].plot(test_losses, label='Test Loss')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].set_title(\"Loss Curve\")\n",
    "\n",
    "    axs[0, 1].plot(train_accuracies, label='Train Accuracy')\n",
    "    axs[0, 1].plot(test_accuracies, label='Test Accuracy')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].set_title(\"Accuracy Curve\")\n",
    "\n",
    "    axs[1, 0].plot(v2_accs, label='2-class Accuracy')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].set_title(\"2-Class Accuracy Curve\")\n",
    "\n",
    "    axs[1, 1].plot(v4_accs, label='4-class Accuracy')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].set_title(\"4-Class Accuracy Curve\")\n",
    "\n",
    "    axs[2, 0].plot(v10_accs, label='10-class Accuracy')\n",
    "    axs[2, 0].legend()\n",
    "    axs[2, 0].set_title(\"10-Class Accuracy Curve\")\n",
    "\n",
    "    info_text = (f\"Best Model Info (from Epoch {best_epoch_info['epoch']}):\\n\"\n",
    "                # f\"Train Loss: {best_epoch_info['train_loss']:.4f}\\n\"\n",
    "                # f\"Train Accuracy: {best_epoch_info['train_accuracy']:.4f}\\n\"\n",
    "                f\"Test Loss: {best_epoch_info['test_loss']:.4f}\\n\"\n",
    "                f\"Test Accuracy: {best_epoch_info['test_accuracy']:.4f}\\n\"\n",
    "                f\"v2_acc:{best_epoch_info['v2_acc']:.4f}\\n\"\n",
    "                f\"v4_acc:{best_epoch_info['v4_acc']:.4f}\\n\"\n",
    "                f\"v10_acc:{best_epoch_info['v10_acc']:.4f}\")\n",
    "\n",
    "    axs[2, 1].axis('off')  \n",
    "    axs[2, 1].text(0.5, 0.5, info_text, fontsize=10, ha='center', va='center', transform=axs[2, 1].transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.suptitle('pos_img_text', fontsize=16, y=1.05)\n",
    "    plt.savefig('pos_img_text')\n",
    "    logger.finish()\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    Encoder_list = ['EEGNetv4_Encoder', 'ATCNet_Encoder', 'EEGConformer_Encoder', 'EEGITNet_Encoder', 'ShallowFBCSPNet_Encoder'] \n",
    "    config = {\n",
    "    \"data_path\": \"/home/ldy/Workspace/THINGS/Preprocessed_data_250Hz\",\n",
    "    \"project\": \"train_pos_img_text_rep\",\n",
    "    \"entity\": \"sustech_rethinkingbci\",\n",
    "    \"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "    \"lr\": 3e-4,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 1024,\n",
    "    \"logger\": True,\n",
    "    \"insubject\":True,\n",
    "    \"encoder_type\":'ATM_S',\n",
    "    \"img_encoder\": 'Proj_img'\n",
    "    }\n",
    "    \n",
    "    eeg_model = globals()[config['encoder_type']]()\n",
    "    img_model = globals()[config['img_encoder']]()\n",
    "    optimizer = torch.optim.AdamW(itertools.chain(eeg_model.parameters(), img_model.parameters()), lr=config['lr'])            \n",
    "    print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()])+sum([p.numel() for p in img_model.parameters()]))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # eeg_model = eeg_model.to(device)\n",
    "    # img_model = img_model.to(device)\n",
    "    data_path = config['data_path']\n",
    "    subjects = ['sub-01', 'sub-02', 'sub-03', 'sub-04',  'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']            \n",
    "    \n",
    "    # subjects = ['sub-08']            \n",
    "    if config['insubject']:\n",
    "        for sub in subjects:                    \n",
    "            # train_dataset = EEGDataset(data_path, subjects=[sub], train=True)\n",
    "            test_dataset = EEGDataset(data_path, subjects=[sub], train=False)\n",
    "            # train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "            train_loader = None\n",
    "            text_features_train_all = None\n",
    "            img_features_train_all = None\n",
    "            # text_features_train_all = train_dataset.text_features\n",
    "            text_features_test_all = test_dataset.text_features\n",
    "            # img_features_train_all = train_dataset.img_features\n",
    "            img_features_test_all = test_dataset.img_features        \n",
    "            generated_features_all = test_dataset.generated_features\n",
    "            results = main_train_loop(sub, eeg_model, img_model, train_loader, test_loader, optimizer, device, \n",
    "                            text_features_train_all, text_features_test_all, img_features_train_all, img_features_test_all, generated_features_all, config, logger=config['logger'])\n",
    "            \n",
    "            # Save results to a CSV file\n",
    "            results_file = f'./tab_generation_metric_outputs/{(config[\"encoder_type\"])}_{sub}.csv'\n",
    "            with open(results_file, 'w', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=results[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(results)\n",
    "            print(f'Results saved to {results_file}')\n",
    "            \n",
    "    else:                 \n",
    "        for sub in subjects:\n",
    "            train_dataset = EEGDataset(data_path, exclude_subject=sub, train=True)\n",
    "            test_dataset = EEGDataset(data_path, exclude_subject=sub, train=False)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "            text_features_train_all = train_dataset.text_features\n",
    "            text_features_test_all = test_dataset.text_features\n",
    "            img_features_train_all = train_dataset.img_features\n",
    "            img_features_test_all = test_dataset.img_features\n",
    "\n",
    "            results = main_train_loop(sub, eeg_model, img_model, train_loader, test_loader, optimizer, device, \n",
    "                        text_features_train_all, text_features_test_all, img_features_train_all, img_features_test_all, config, logger=config['logger'])\n",
    "\n",
    "            # Save results to a CSV file\n",
    "            results_file = f'./tab_generation_metric_outputs/{config[\"encoder_type\"]}_cross_exclude_{sub}.csv'\n",
    "            with open(results_file, 'w', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=results[0].keys())\n",
    "                writer.writeheader()\n",
    "                writer.writerows(results)\n",
    "                print(f'Results saved to {results_file}')\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
