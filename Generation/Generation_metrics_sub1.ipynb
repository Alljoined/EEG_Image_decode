{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from eegdatasets_leaveone import EEGDataset\n",
    "\n",
    "from ATM_S_reconstruction_scale_0_1000 import ATM_S_reconstruction_scale_0_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to get EEG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eegfeatures(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k, mode):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    alpha = 0.9\n",
    "\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, _, text_features, _, img_features) in enumerate(dataloader):\n",
    "            eeg_data = eeg_data.to(device)\n",
    "            eeg_data = eeg_data[:, :, :250]\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            eeg_features = eegmodel(eeg_data).float()\n",
    "            features_list.append(eeg_features)\n",
    "            logit_scale = eegmodel.logit_scale \n",
    "                   \n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)     \n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            _ = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)    \n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "\n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                logits_single = logits_img\n",
    "\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                total += 1\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"ATM_S_eeg_features_{sub}_{mode}.pt\")  # Save features as .pt file\n",
    "\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Run\n",
    "\n",
    "- Note: If you've already run ATM_S_reconstruction_scale_0_1000.py, use the .pth file in Generation/models/contrast folder, if not, use Jonathan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized EEG Attention\n",
      "number of parameters: 3655541\n",
      "self.subjects ['sub-01']\n",
      "exclude_subject None\n",
      "Data tensor shape: torch.Size([200, 63, 250]), label tensor shape: torch.Size([200]), text length: 200, image length: 200\n",
      "features_tensor torch.Size([200, 1024])\n",
      " - Test Loss: 7.0661, Test Accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"data_path\": \"/srv/eeg_reconstruction/shared/things_eeg_2/Preprocessed_data_250Hz\",\n",
    "    \"project\": \"EEG_image_generation\",\n",
    "    \"entity\": \"alljoined1\",\n",
    "    \"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "    \"lr\": 3e-4,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 1024,\n",
    "    \"logger\": True,\n",
    "    \"encoder_type\":'ATM_S_reconstruction_scale_0_1000',\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = config['data_path']\n",
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "\n",
    "eeg_model = ATM_S_reconstruction_scale_0_1000(63, 250)\n",
    "print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()]))\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/Reconstruction/models/contrast/sub-08/01-30_00-44/40.pth\"))\n",
    "\n",
    "### If you've ran ATM_S_reconstruction_scale_0_1000.py use:\n",
    "# eeg_model.load_state_dict(torch.load(\"models/contrast/ATM_S_reconstruction_scale_0_1000/05-13_20-16/sub-01/40.pth\"))\n",
    "\n",
    "# Otherwise use:\n",
    "eeg_model.load_state_dict(torch.load(\"/srv/eeg_reconstruction/jonathan/EEG_Image_decode/Generation/models/contrast/ATM_S_reconstruction_scale_0_1000/05-13_20-16/sub-01/40.pth\"))\n",
    "\n",
    "eeg_model = eeg_model.to(device)\n",
    "sub = 'sub-01'\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "test_dataset = EEGDataset(data_path, subjects=[sub], train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "\n",
    "text_features_test_all = test_dataset.text_features\n",
    "img_features_test_all = test_dataset.img_features\n",
    "\n",
    "test_loss, test_accuracy,labels, eeg_features_test = get_eegfeatures(sub, eeg_model, test_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"test\")\n",
    "print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)\n",
    "emb_eeg = torch.load(f'/srv/eeg_reconstruction/jonathan/EEG_Image_decode/Generation/ATM_S_eeg_features_sub-01_train.pt')\n",
    "# emb_eeg = torch.load(f'ATM_S_eeg_features_sub-01_train.pt')\n",
    "# emb_eeg_test = torch.load(f'ATM_S_eeg_features_sub-01_test.pt')\n",
    "emb_eeg_test = torch.load(f'/srv/eeg_reconstruction/jonathan/EEG_Image_decode/Generation/ATM_S_eeg_features_sub-01_test.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([66160, 1024]), torch.Size([200, 1024]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_eeg.shape, emb_eeg_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.embeddings import Timesteps, TimestepEmbedding\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, c_embeddings=None, h_embeddings=None, h_embeds_uncond=None, cond_sampling_rate=0.5):\n",
    "        self.c_embeddings = c_embeddings\n",
    "        self.h_embeddings = h_embeddings\n",
    "        self.N_cond = 0 if self.h_embeddings is None else len(self.h_embeddings)\n",
    "        self.h_embeds_uncond = h_embeds_uncond\n",
    "        self.N_uncond = 0 if self.h_embeds_uncond is None else len(self.h_embeds_uncond)\n",
    "        self.cond_sampling_rate = cond_sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N_cond\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"c_embedding\": self.c_embeddings[idx],\n",
    "            \"h_embedding\": self.h_embeddings[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion Prior UNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionPriorUNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            embed_dim=1024, \n",
    "            cond_dim=42,\n",
    "            hidden_dim=[1024, 512, 256, 128, 64],\n",
    "            time_embed_dim=512,\n",
    "            act_fn=nn.SiLU,\n",
    "            dropout=0.0,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 1. time embedding\n",
    "        self.time_proj = Timesteps(time_embed_dim, True, 0)\n",
    "\n",
    "        # 2. conditional embedding \n",
    "        # to 3.2, 3,3\n",
    "\n",
    "        # 3. prior mlp\n",
    "\n",
    "        # 3.1 input\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim[0]),\n",
    "            nn.LayerNorm(hidden_dim[0]),\n",
    "            act_fn(),\n",
    "        )\n",
    "\n",
    "        # 3.2 hidden encoder\n",
    "        self.num_layers = len(hidden_dim)\n",
    "        self.encode_time_embedding = nn.ModuleList(\n",
    "            [TimestepEmbedding(\n",
    "                time_embed_dim,\n",
    "                hidden_dim[i],\n",
    "            ) for i in range(self.num_layers-1)]\n",
    "        ) # d_0, ..., d_{n-1}\n",
    "        self.encode_cond_embedding = nn.ModuleList(\n",
    "            [nn.Linear(cond_dim, hidden_dim[i]) for i in range(self.num_layers-1)]\n",
    "        )\n",
    "        self.encode_layers = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                    nn.Linear(hidden_dim[i], hidden_dim[i+1]),\n",
    "                    nn.LayerNorm(hidden_dim[i+1]),\n",
    "                    act_fn(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ) for i in range(self.num_layers-1)]\n",
    "        )\n",
    "\n",
    "        # 3.3 hidden decoder\n",
    "        self.decode_time_embedding = nn.ModuleList(\n",
    "            [TimestepEmbedding(\n",
    "                time_embed_dim,\n",
    "                hidden_dim[i],\n",
    "            ) for i in range(self.num_layers-1,0,-1)]\n",
    "        ) # d_{n}, ..., d_1\n",
    "        self.decode_cond_embedding = nn.ModuleList(\n",
    "            [nn.Linear(cond_dim, hidden_dim[i]) for i in range(self.num_layers-1,0,-1)]\n",
    "        )\n",
    "        self.decode_layers = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                    nn.Linear(hidden_dim[i], hidden_dim[i-1]),\n",
    "                    nn.LayerNorm(hidden_dim[i-1]),\n",
    "                    act_fn(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ) for i in range(self.num_layers-1,0,-1)]\n",
    "        )\n",
    "\n",
    "        # 3.4 output\n",
    "        self.output_layer = nn.Linear(hidden_dim[0], embed_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        # x (batch_size, embed_dim)\n",
    "        # t (batch_size, )\n",
    "        # c (batch_size, cond_dim)\n",
    "\n",
    "        # 1. time embedding\n",
    "        t = self.time_proj(t) # (batch_size, time_embed_dim)\n",
    "\n",
    "        # 2. conditional embedding \n",
    "        # to 3.2, 3.3\n",
    "\n",
    "        # 3. prior mlp\n",
    "\n",
    "        # 3.1 input\n",
    "        x = self.input_layer(x) \n",
    "\n",
    "        # 3.2 hidden encoder\n",
    "        hidden_activations = []\n",
    "        for i in range(self.num_layers-1):\n",
    "            hidden_activations.append(x)\n",
    "            t_emb = self.encode_time_embedding[i](t) \n",
    "            c_emb = self.encode_cond_embedding[i](c) if c is not None else 0\n",
    "            x = x + t_emb + c_emb\n",
    "            x = self.encode_layers[i](x)\n",
    "        \n",
    "        # 3.3 hidden decoder\n",
    "        for i in range(self.num_layers-1):\n",
    "            t_emb = self.decode_time_embedding[i](t)\n",
    "            c_emb = self.decode_cond_embedding[i](c) if c is not None else 0\n",
    "            x = x + t_emb + c_emb\n",
    "            x = self.decode_layers[i](x)\n",
    "            x += hidden_activations[-1-i]\n",
    "            \n",
    "        # 3.4 output\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipe:\n",
    "    def __init__(self, diffusion_prior=None, scheduler=None, device='cuda'):\n",
    "        self.diffusion_prior = diffusion_prior.to(device)\n",
    "        \n",
    "        if scheduler is None:\n",
    "            from diffusers.schedulers import DDPMScheduler\n",
    "            self.scheduler = DDPMScheduler() \n",
    "        else:\n",
    "            self.scheduler = scheduler\n",
    "            \n",
    "        self.device = device\n",
    "        \n",
    "    def train(self, dataloader, num_epochs=10, learning_rate=1e-4):\n",
    "        self.diffusion_prior.train()\n",
    "        device = self.device\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        optimizer = optim.Adam(self.diffusion_prior.parameters(), lr=learning_rate)\n",
    "        \n",
    "        from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "        lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=500,\n",
    "            num_training_steps=(len(dataloader) * num_epochs),\n",
    "        )\n",
    "\n",
    "        num_train_timesteps = self.scheduler.config.num_train_timesteps\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_sum = 0\n",
    "            for batch in dataloader:\n",
    "                c_embeds = batch['c_embedding'].to(device) if 'c_embedding' in batch.keys() else None\n",
    "                h_embeds = batch['h_embedding'].to(device)\n",
    "                N = h_embeds.shape[0]\n",
    "\n",
    "                # 1. randomly replecing c_embeds to None\n",
    "                if torch.rand(1) < 0.1:\n",
    "                    c_embeds = None\n",
    "\n",
    "                # 2. Generate noisy embeddings as input\n",
    "                noise = torch.randn_like(h_embeds)\n",
    "\n",
    "                # 3. sample timestep\n",
    "                timesteps = torch.randint(0, num_train_timesteps, (N,), device=device)\n",
    "\n",
    "                # 4. add noise to h_embedding\n",
    "                perturbed_h_embeds = self.scheduler.add_noise(\n",
    "                    h_embeds,\n",
    "                    noise,\n",
    "                    timesteps\n",
    "                ) # (batch_size, embed_dim), (batch_size, )\n",
    "\n",
    "                # 5. predict noise\n",
    "                noise_pre = self.diffusion_prior(perturbed_h_embeds, timesteps, c_embeds)\n",
    "                \n",
    "                # 6. loss function weighted by sigma\n",
    "                loss = criterion(noise_pre, noise) # (batch_size,)\n",
    "                loss = (loss).mean()\n",
    "                            \n",
    "                # 7. update parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.diffusion_prior.parameters(), 1.0)\n",
    "                lr_scheduler.step()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "            loss_epoch = loss_sum / len(dataloader)\n",
    "            print(f'epoch: {epoch}, loss: {loss_epoch}')\n",
    "\n",
    "    def generate(\n",
    "            self, \n",
    "            c_embeds=None, \n",
    "            num_inference_steps=50, \n",
    "            timesteps=None,\n",
    "            guidance_scale=5.0,\n",
    "            generator=None\n",
    "        ):\n",
    "        # c_embeds (batch_size, cond_dim)\n",
    "        self.diffusion_prior.eval()\n",
    "        N = c_embeds.shape[0] if c_embeds is not None else 1\n",
    "\n",
    "        # 1. Prepare timesteps\n",
    "        from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import retrieve_timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, self.device, timesteps)\n",
    "\n",
    "        # 2. Prepare c_embeds\n",
    "        if c_embeds is not None:\n",
    "            c_embeds = c_embeds.to(self.device)\n",
    "\n",
    "        # 3. Prepare noise\n",
    "        h_t = torch.randn(N, self.diffusion_prior.embed_dim, generator=generator, device=self.device)\n",
    "\n",
    "        # 4. denoising loop\n",
    "        for _, t in tqdm(enumerate(timesteps)):\n",
    "            t = torch.ones(h_t.shape[0], dtype=torch.float, device=self.device) * t\n",
    "\n",
    "            # 4.1 noise prediction\n",
    "            if guidance_scale == 0 or c_embeds is None:\n",
    "                noise_pred = self.diffusion_prior(h_t, t)\n",
    "            else:\n",
    "                noise_pred_cond = self.diffusion_prior(h_t, t, c_embeds)\n",
    "                noise_pred_uncond = self.diffusion_prior(h_t, t)\n",
    "\n",
    "                # perform classifier-free guidance\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "            # 4.2 compute the previous noisy sample h_t -> h_{t-1}\n",
    "            h_t = self.scheduler.step(noise_pred, t.long().item(), h_t, generator=generator).prev_sample\n",
    "        \n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9675648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 1.1344700428155752\n",
      "epoch: 1, loss: 0.9299891077555142\n",
      "epoch: 2, loss: 0.7368258384557871\n",
      "epoch: 3, loss: 0.589963946892665\n",
      "epoch: 4, loss: 0.48536915091367866\n",
      "epoch: 5, loss: 0.40523732304573057\n",
      "epoch: 6, loss: 0.35007531688763543\n",
      "epoch: 7, loss: 0.31583392024040224\n",
      "epoch: 8, loss: 0.2946344316005707\n",
      "epoch: 9, loss: 0.28302158071444583\n"
     ]
    }
   ],
   "source": [
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=emb_eeg, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "dl = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=32)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)\n",
    "\n",
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' #s 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "pipe.train(dl, num_epochs=10, learning_rate=1e-3) # to 0.142 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = f'./fintune_ckpts/{config[\"encoder_type\"]}/{sub}/{model_name}.pt'\n",
    "\n",
    "directory = os.path.dirname(save_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "torch.save(pipe.diffusion_prior.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.diffusion_prior.load_state_dict(torch.load(\"fintune_ckpts/ATM_S_reconstruction_scale_0_1000/sub-01/diffusion_prior.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.stable_diffusion_xl.pipeline_stable_diffusion_xl import *\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "@replace_example_docstring(EXAMPLE_DOC_STRING)\n",
    "def generate_ip_adapter_embeds(\n",
    "    self,\n",
    "    prompt: Union[str, List[str]] = None,\n",
    "    prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "    height: Optional[int] = None,\n",
    "    width: Optional[int] = None,\n",
    "    num_inference_steps: int = 50,\n",
    "    timesteps: List[int] = None,\n",
    "    denoising_end: Optional[float] = None,\n",
    "    guidance_scale: float = 5.0,\n",
    "    negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "    negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "    num_images_per_prompt: Optional[int] = 1,\n",
    "    eta: float = 0.0,\n",
    "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "    latents: Optional[torch.FloatTensor] = None,\n",
    "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    ip_adapter_image: Optional[PipelineImageInput] = None,\n",
    "    ip_adapter_embeds: Optional[torch.FloatTensor] = None,\n",
    "    output_type: Optional[str] = \"pil\",\n",
    "    return_dict: bool = True,\n",
    "    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    guidance_rescale: float = 0.0,\n",
    "    original_size: Optional[Tuple[int, int]] = None,\n",
    "    crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "    target_size: Optional[Tuple[int, int]] = None,\n",
    "    negative_original_size: Optional[Tuple[int, int]] = None,\n",
    "    negative_crops_coords_top_left: Tuple[int, int] = (0, 0),\n",
    "    negative_target_size: Optional[Tuple[int, int]] = None,\n",
    "    clip_skip: Optional[int] = None,\n",
    "    callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
    "    callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "    **kwargs,\n",
    "):\n",
    "    r\"\"\"\n",
    "    Examples:\n",
    "    \"\"\"\n",
    "\n",
    "    callback = kwargs.pop(\"callback\", None)\n",
    "    callback_steps = kwargs.pop(\"callback_steps\", None)\n",
    "\n",
    "    if callback is not None:\n",
    "        deprecate(\n",
    "            \"callback\",\n",
    "            \"1.0.0\",\n",
    "            \"Passing `callback` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "        )\n",
    "    if callback_steps is not None:\n",
    "        deprecate(\n",
    "            \"callback_steps\",\n",
    "            \"1.0.0\",\n",
    "            \"Passing `callback_steps` as an input argument to `__call__` is deprecated, consider use `callback_on_step_end`\",\n",
    "        )\n",
    "\n",
    "    # 0. Default height and width to unet\n",
    "    height = height or self.default_sample_size * self.vae_scale_factor\n",
    "    width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "    original_size = original_size or (height, width)\n",
    "    target_size = target_size or (height, width)\n",
    "\n",
    "    # 1. Check inputs. Raise error if not correct\n",
    "    self.check_inputs(\n",
    "        prompt,\n",
    "        prompt_2,\n",
    "        height,\n",
    "        width,\n",
    "        callback_steps,\n",
    "        negative_prompt,\n",
    "        negative_prompt_2,\n",
    "        prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds,\n",
    "        callback_on_step_end_tensor_inputs,\n",
    "    )\n",
    "\n",
    "    self._guidance_scale = guidance_scale\n",
    "    self._guidance_rescale = guidance_rescale\n",
    "    self._clip_skip = clip_skip\n",
    "    self._cross_attention_kwargs = cross_attention_kwargs\n",
    "    self._denoising_end = denoising_end\n",
    "\n",
    "    # 2. Define call parameters\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    device = self._execution_device\n",
    "\n",
    "    # 3. Encode input prompt\n",
    "    lora_scale = (\n",
    "        self.cross_attention_kwargs.get(\"scale\", None) if self.cross_attention_kwargs is not None else None\n",
    "    )\n",
    "\n",
    "    (\n",
    "        prompt_embeds,\n",
    "        negative_prompt_embeds,\n",
    "        pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds,\n",
    "    ) = self.encode_prompt(\n",
    "        prompt=prompt,\n",
    "        prompt_2=prompt_2,\n",
    "        device=device,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "        negative_prompt=negative_prompt,\n",
    "        negative_prompt_2=negative_prompt_2,\n",
    "        prompt_embeds=prompt_embeds,\n",
    "        negative_prompt_embeds=negative_prompt_embeds,\n",
    "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "        lora_scale=lora_scale,\n",
    "        clip_skip=self.clip_skip,\n",
    "    )\n",
    "\n",
    "    # 4. Prepare timesteps\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, device, timesteps)\n",
    "\n",
    "    # 5. Prepare latent variables\n",
    "    num_channels_latents = self.unet.config.in_channels\n",
    "    latents = self.prepare_latents(\n",
    "        batch_size * num_images_per_prompt,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        prompt_embeds.dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents,\n",
    "    )\n",
    "\n",
    "    # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "    # 7. Prepare added time ids & embeddings\n",
    "    add_text_embeds = pooled_prompt_embeds\n",
    "    if self.text_encoder_2 is None:\n",
    "        text_encoder_projection_dim = int(pooled_prompt_embeds.shape[-1])\n",
    "    else:\n",
    "        text_encoder_projection_dim = self.text_encoder_2.config.projection_dim\n",
    "\n",
    "    add_time_ids = self._get_add_time_ids(\n",
    "        original_size,\n",
    "        crops_coords_top_left,\n",
    "        target_size,\n",
    "        dtype=prompt_embeds.dtype,\n",
    "        text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "    )\n",
    "    if negative_original_size is not None and negative_target_size is not None:\n",
    "        negative_add_time_ids = self._get_add_time_ids(\n",
    "            negative_original_size,\n",
    "            negative_crops_coords_top_left,\n",
    "            negative_target_size,\n",
    "            dtype=prompt_embeds.dtype,\n",
    "            text_encoder_projection_dim=text_encoder_projection_dim,\n",
    "        )\n",
    "    else:\n",
    "        negative_add_time_ids = add_time_ids\n",
    "\n",
    "    if self.do_classifier_free_guidance:\n",
    "        prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "        add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)\n",
    "        add_time_ids = torch.cat([negative_add_time_ids, add_time_ids], dim=0)\n",
    "\n",
    "    prompt_embeds = prompt_embeds.to(device)\n",
    "    add_text_embeds = add_text_embeds.to(device)\n",
    "    add_time_ids = add_time_ids.to(device).repeat(batch_size * num_images_per_prompt, 1)\n",
    "\n",
    "    if ip_adapter_image is not None:\n",
    "        image_embeds, negative_image_embeds = self.encode_image(ip_adapter_image, device, num_images_per_prompt)\n",
    "        if self.do_classifier_free_guidance:\n",
    "            image_embeds = torch.cat([negative_image_embeds, image_embeds])\n",
    "            image_embeds = image_embeds.to(device)\n",
    "    \n",
    "    if ip_adapter_embeds is not None:\n",
    "        image_embeds = ip_adapter_embeds.to(device=device, dtype=prompt_embeds.dtype)\n",
    "        if self.do_classifier_free_guidance:\n",
    "            negative_image_embeds = torch.zeros_like(image_embeds)\n",
    "            image_embeds = torch.cat([negative_image_embeds, image_embeds])\n",
    "            image_embeds = image_embeds.to(device)\n",
    "\n",
    "    # 8. Denoising loop\n",
    "    num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "\n",
    "    # 8.1 Apply denoising_end\n",
    "    if (\n",
    "        self.denoising_end is not None\n",
    "        and isinstance(self.denoising_end, float)\n",
    "        and self.denoising_end > 0\n",
    "        and self.denoising_end < 1\n",
    "    ):\n",
    "        discrete_timestep_cutoff = int(\n",
    "            round(\n",
    "                self.scheduler.config.num_train_timesteps\n",
    "                - (self.denoising_end * self.scheduler.config.num_train_timesteps)\n",
    "            )\n",
    "        )\n",
    "        num_inference_steps = len(list(filter(lambda ts: ts >= discrete_timestep_cutoff, timesteps)))\n",
    "        timesteps = timesteps[:num_inference_steps]\n",
    "\n",
    "    # 9. Optionally get Guidance Scale Embedding\n",
    "    timestep_cond = None\n",
    "    if self.unet.config.time_cond_proj_dim is not None:\n",
    "        guidance_scale_tensor = torch.tensor(self.guidance_scale - 1).repeat(batch_size * num_images_per_prompt)\n",
    "        timestep_cond = self.get_guidance_scale_embedding(\n",
    "            guidance_scale_tensor, embedding_dim=self.unet.config.time_cond_proj_dim\n",
    "        ).to(device=device, dtype=latents.dtype)\n",
    "\n",
    "    self._num_timesteps = len(timesteps)\n",
    "    with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "\n",
    "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            added_cond_kwargs = {\"text_embeds\": add_text_embeds, \"time_ids\": add_time_ids}\n",
    "            if ip_adapter_image is not None or ip_adapter_embeds is not None:\n",
    "                added_cond_kwargs[\"image_embeds\"] = image_embeds\n",
    "            noise_pred = self.unet(\n",
    "                latent_model_input,\n",
    "                t,\n",
    "                encoder_hidden_states=prompt_embeds,\n",
    "                timestep_cond=timestep_cond,\n",
    "                cross_attention_kwargs=self.cross_attention_kwargs,\n",
    "                added_cond_kwargs=added_cond_kwargs,\n",
    "                return_dict=False,\n",
    "            )[0]\n",
    "\n",
    "            # perform guidance\n",
    "            if self.do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:\n",
    "                # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf\n",
    "                noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n",
    "\n",
    "            if callback_on_step_end is not None:\n",
    "                callback_kwargs = {}\n",
    "                for k in callback_on_step_end_tensor_inputs:\n",
    "                    callback_kwargs[k] = locals()[k]\n",
    "                callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                latents = callback_outputs.pop(\"latents\", latents)\n",
    "                prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "                add_text_embeds = callback_outputs.pop(\"add_text_embeds\", add_text_embeds)\n",
    "                negative_pooled_prompt_embeds = callback_outputs.pop(\n",
    "                    \"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds\n",
    "                )\n",
    "                add_time_ids = callback_outputs.pop(\"add_time_ids\", add_time_ids)\n",
    "                negative_add_time_ids = callback_outputs.pop(\"negative_add_time_ids\", negative_add_time_ids)\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "                if callback is not None and i % callback_steps == 0:\n",
    "                    step_idx = i // getattr(self.scheduler, \"order\", 1)\n",
    "                    callback(step_idx, t, latents)\n",
    "\n",
    "            if XLA_AVAILABLE:\n",
    "                xm.mark_step()\n",
    "\n",
    "    if not output_type == \"latent\":\n",
    "        # make sure the VAE is in float32 mode, as it overflows in float16\n",
    "        needs_upcasting = self.vae.dtype == torch.float16 and self.vae.config.force_upcast\n",
    "\n",
    "        if needs_upcasting:\n",
    "            self.upcast_vae()\n",
    "            latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\n",
    "\n",
    "        image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "\n",
    "        # cast back to fp16 if needed\n",
    "        if needs_upcasting:\n",
    "            self.vae.to(dtype=torch.float16)\n",
    "    else:\n",
    "        image = latents\n",
    "\n",
    "    if not output_type == \"latent\":\n",
    "        # apply watermark if available\n",
    "        if self.watermark is not None:\n",
    "            image = self.watermark.apply_watermark(image)\n",
    "\n",
    "        image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "    # Offload all models\n",
    "    self.maybe_free_model_hooks()\n",
    "\n",
    "    if not return_dict:\n",
    "        return (image,)\n",
    "\n",
    "    return StableDiffusionXLPipelineOutput(images=image)\n",
    "\n",
    "def encode_image(image, image_encoder, feature_extractor, num_images_per_prompt=1, device='cuda'):\n",
    "    dtype = next(image_encoder.parameters()).dtype\n",
    "\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image = feature_extractor(image, return_tensors=\"pt\").pixel_values # [1, 3, 224, 224]\n",
    "    \n",
    "    image = image.to(device=device, dtype=dtype)\n",
    "    image_embeds = image_encoder(image).image_embeds # (1, 1024)\n",
    "    image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0) # (num_images_per_prompt, 1024)\n",
    "\n",
    "    return image_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to generate images from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator4Embeds:\n",
    "    def __init__(self, num_inference_steps=1, device='cuda') -> None:\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        self.dtype = torch.float16\n",
    "        self.device = device\n",
    "        \n",
    "        # path = '/home/weichen/.cache/huggingface/hub/models--stabilityai--sdxl-turbo/snapshots/f4b0486b498f84668e828044de1d0c8ba486e05b'\n",
    "        # path = \"/home/ldy/Workspace/sdxl-turbo/f4b0486b498f84668e828044de1d0c8ba486e05b\"\n",
    "        pipe = DiffusionPipeline.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "        # pipe = DiffusionPipeline.from_pretrained(path, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "        pipe.to(device)\n",
    "        pipe.generate_ip_adapter_embeds = generate_ip_adapter_embeds.__get__(pipe)\n",
    "        # load ip adapter\n",
    "        pipe.load_ip_adapter(\n",
    "            \"h94/IP-Adapter\", subfolder=\"sdxl_models\", \n",
    "            weight_name=\"ip-adapter_sdxl_vit-h.safetensors\", \n",
    "            torch_dtype=torch.float16)\n",
    "        # set ip_adapter scale (defauld is 1)\n",
    "        pipe.set_ip_adapter_scale(1)\n",
    "        self.pipe = pipe\n",
    "\n",
    "    def generate(self, image_embeds, text_prompt='', generator=None):\n",
    "        image_embeds = image_embeds.to(device=self.device, dtype=self.dtype)\n",
    "        pipe = self.pipe\n",
    "\n",
    "        # generate image with image prompt - ip_adapter_embeds\n",
    "        image = pipe.generate_ip_adapter_embeds(\n",
    "            prompt=text_prompt, \n",
    "            ip_adapter_embeds=image_embeds, \n",
    "            num_inference_steps=self.num_inference_steps,\n",
    "            guidance_scale=0.0,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Save Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/mambaforge/envs/image_decode/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625428fdfcbd44f1beae9afad1ec9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.subjects ['sub-01']\n",
      "exclude_subject None\n",
      "Data tensor shape: torch.Size([200, 63, 250]), label tensor shape: torch.Size([200]), text length: 200, image length: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 215.38it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3bc3f7b1ae48adb3821ae7ef380f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/models/embeddings.py:898: FutureWarning: You have passed a tensor as `image_embeds`.This is deprecated and will be removed in a future release. Please make sure to update your script to pass `image_embeds` as a list of tensors to supress this warning.\n",
      "  deprecate(\"image_embeds not a list\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/ethan/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 260.00 MiB. GPU  (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/ethan/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/ethan/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 516.00 MiB. GPU  (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to Generation/generated_imgs/sub-01/This picture is aircraft_carrier/0.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1010b9d7de59447786bb5e024acc2ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to Generation/generated_imgs/sub-01/This picture is aircraft_carrier/1.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7bc7d08f254dae93fce6b530071b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to Generation/generated_imgs/sub-01/This picture is aircraft_carrier/2.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eab9c1a5fb448a5a4aa701638e5b154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m h \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mgenerate(c_embeds\u001b[38;5;241m=\u001b[39meeg_embeds, num_inference_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, guidance_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Construct the save path for each image\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtexts[k]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[36], line 27\u001b[0m, in \u001b[0;36mGenerator4Embeds.generate\u001b[0;34m(self, image_embeds, text_prompt, generator)\u001b[0m\n\u001b[1;32m     24\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# generate image with image prompt - ip_adapter_embeds\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_ip_adapter_embeds\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 289\u001b[0m, in \u001b[0;36mgenerate_ip_adapter_embeds\u001b[0;34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupcast_vae()\n\u001b[1;32m    287\u001b[0m     latents \u001b[38;5;241m=\u001b[39m latents\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mpost_quant_conv\u001b[38;5;241m.\u001b[39mparameters()))\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 289\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# cast back to fp16 if needed\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_upcasting:\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:304\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    302\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:275\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiled_decode(z, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[1;32m    274\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 275\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:338\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[0;32m--> 338\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2741\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2740\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n\u001b[0;32m-> 2741\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mupsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/diffusers/models/upsampling.py:183\u001b[0m, in \u001b[0;36mUpsample2D.forward\u001b[0;34m(self, hidden_states, output_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_conv:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 183\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv2d_0(hidden_states)\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/image_decode/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Assuming generator.generate returns a PIL Image\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "\n",
    "_, _, texts, _ = test_dataset.load_data()\n",
    "\n",
    "directory = f\"Generation/generated_imgs/{sub}\"\n",
    "for k in range(200):\n",
    "    eeg_embeds = emb_eeg_test[k:k+1]\n",
    "    h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0)\n",
    "    for j in range(10):\n",
    "        image = generator.generate(h.to(dtype=torch.float16))\n",
    "        # Construct the save path for each image\n",
    "        path = f'{directory}/{texts[k]}/{j}.png'\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        # Save the PIL Image\n",
    "        image.save(path)\n",
    "        print(f'Image saved to {path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
