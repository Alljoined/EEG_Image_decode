{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.environ['http_proxy'] = 'http://10.16.35.10:13390' \n",
    "# os.environ['https_proxy'] = 'http://10.16.35.10:13390' \n",
    "# os.environ['PATH'] += os.pathsep + '/usr/local/texlive/2023/bin/x86_64-linux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "# os.environ['http_proxy'] = 'http://10.16.35.10:13390' \n",
    "# os.environ['https_proxy'] = 'http://10.16.35.10:13390' \n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from eegdatasets_leaveone import EEGDataset\n",
    "from eegencoder import eeg_encoder\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from lavis.models.clip_models.loss import ClipLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from utils import wandb_logger\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model + 1, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[:d_model // 2 + 1])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:d_model // 2])\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:x.size(0), :].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EEGAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(EEGAttention, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(2, 0, 1)  # Change shape to [time_length, batch_size, channel]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output.permute(1, 2, 0)  # Change shape back to [batch_size, channel, time_length]\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "        # revised from shallownet\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.AvgPool2d((1, 51), (1, 5)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(40, 40, (63, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  \n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b, _, _, _ = x.shape\n",
    "        x = x.unsqueeze(1)     \n",
    "        # print(\"x\", x.shape)   \n",
    "        x = self.tsconv(x)\n",
    "        # print(\"tsconv\", x.shape)   \n",
    "        x = self.projection(x)\n",
    "        # print(\"projection\", x.shape)  \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(emb_size),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "        \n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1440, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class Proj_img(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1024, proj_dim=1024, drop_proj=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "class Padding(nn.Module):\n",
    "    def __init__(self, max_length=1000, sampling_rate=250):\n",
    "        super(Padding, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.max_samples = self.max_length * sampling_rate // 1000\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the padding size for each sequence\n",
    "        current_length = x.size(2)\n",
    "        padding_size = self.max_samples - current_length\n",
    "        # Apply zero padding to the end of the sequences if necessary\n",
    "        if padding_size > 0:\n",
    "            x_padded = F.pad(x, (0, padding_size), 'constant', 0)\n",
    "        else:\n",
    "            x_padded = x\n",
    "        return x_padded\n",
    "\n",
    "class ATMS(nn.Module):    \n",
    "    def __init__(self, num_channels=63, sequence_length=250, num_subjects=1, num_features=64, num_latents=1024, num_blocks=1):\n",
    "        super(ATMS, self).__init__()\n",
    "        self.attention_model = EEGAttention(num_channels, nhead=1)   \n",
    "        self.subject_wise_linear = nn.ModuleList([nn.Linear(sequence_length, sequence_length) for _ in range(num_subjects)])\n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg()        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_func = ClipLoss()       \n",
    "        self.masking = DynamicMasking(max_time=1000, max_time_proportion=0.3)\n",
    "        self.padding = Padding(max_length=1000, sampling_rate=250)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x, mask = self.masking(x)\n",
    "        x = self.padding(x)\n",
    "\n",
    "        x = self.attention_model(x)\n",
    "        # print(f'After attention shape: {x.shape}')\n",
    "         \n",
    "        x = self.subject_wise_linear[0](x)\n",
    "        # print(f'After subject-specific linear transformation shape: {x.shape}')\n",
    "        eeg_embedding = self.enc_eeg(x)\n",
    "        \n",
    "        out = self.proj_eeg(eeg_embedding)\n",
    "        return out  \n",
    "    \n",
    "\n",
    "class DynamicMasking(nn.Module):\n",
    "    def __init__(self, sampling_rate=250, min_time=50, max_time=1000, max_time_proportion=0.2):\n",
    "        super(DynamicMasking, self).__init__()\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_time = max_time\n",
    "        self.min_time = min_time\n",
    "        self.max_time_proportion = max_time_proportion\n",
    "        self.time_steps = list(range(min_time, max_time + 1, 50))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Randomly select a max_time, but ensure that max_time=1000ms is selected proportionally\n",
    "        if random.random() < self.max_time_proportion:\n",
    "            selected_time = self.max_time\n",
    "        else:\n",
    "            selected_time = random.choice([t for t in self.time_steps if t != self.max_time])\n",
    "\n",
    "        mask_end_index = selected_time * self.sampling_rate // 1000\n",
    "        mask = torch.ones_like(x, dtype=torch.bool)\n",
    "        mask[:, :, mask_end_index:] = 0  # Mask out the time points from selected_time to 1000ms\n",
    "\n",
    "        x_masked = x * mask\n",
    "        return x_masked, mask\n",
    "\n",
    "    \n",
    "\n",
    "def get_eegfeatures(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k, mode):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.9\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    ridge_lambda = 0.1\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            eeg_data = eeg_data.to(device)\n",
    "            eeg_data = eeg_data[:, :, :int(250*0.80)]\n",
    "            print(\"eeg_data\", eeg_data.shape)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            eeg_features = eegmodel(eeg_data).float()\n",
    "            features_list.append(eeg_features)\n",
    "            logit_scale = eegmodel.logit_scale \n",
    "                   \n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"eeg_features\", eeg_features.shape)\n",
    "            # print(torch.std(eeg_features, dim=-1))\n",
    "            # print(torch.std(img_features, dim=-1))\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)            \n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                total += 1\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            # print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"masked_ATM_S_eeg_features_0_800_{sub}_{mode}.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "config = {\n",
    "\"data_path\": \"/home/ldy/Workspace/THINGS/Preprocessed_data_250Hz\",\n",
    "\"project\": \"train_pos_img_text_rep\",\n",
    "\"entity\": \"sustech_rethinkingbci\",\n",
    "\"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "\"lr\": 3e-4,\n",
    "\"epochs\": 50,\n",
    "\"batch_size\": 1024,\n",
    "\"logger\": True,\n",
    "\"encoder_type\":'ATM_S_reconstruction_scale_0_1000',\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_path = config['data_path']\n",
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "eeg_model = ATMS(63, 250)\n",
    "print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()]))\n",
    "\n",
    "#####################################################################################\n",
    "# eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/Reconstruction/models/contrast/sub-08/01-30_00-44/40.pth\"))\n",
    "eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/BrainAligning_retrieval/models/masked/ATMS/sub-08/02-20_15-35/40.pth\"))\n",
    "eeg_model.to(device)\n",
    "sub = 'sub-08'\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "test_dataset = EEGDataset(data_path, subjects= [sub], train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = test_dataset.text_features\n",
    "img_features_test_all = test_dataset.img_features\n",
    "test_loss, test_accuracy,labels, eeg_features_test = get_eegfeatures(sub, eeg_model, test_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"test\")\n",
    "print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################################\n",
    "# train_dataset = EEGDataset(data_path, subjects= [sub], train=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "# text_features_test_all = train_dataset.text_features\n",
    "# img_features_test_all = train_dataset.img_features\n",
    "\n",
    "# train_loss, train_accuracy, labels, eeg_features_train = get_eegfeatures(sub, eeg_model, train_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"train\")\n",
    "# print(f\" - Test Loss: {train_loss:.4f}, Test Accuracy: {train_accuracy:.4f}\")\n",
    "# #####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import sys\n",
    "from diffusion_prior import *\n",
    "from custom_pipeline import *\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" \n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load eeg and image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image embeddings\n",
    "# data = torch.load('/home/ldy/Workspace/THINGS/CLIP/ViT-H-14_features_test.pt', map_location='cuda:3')\n",
    "# emb_img_test = data['img_features']\n",
    "\n",
    "# # load image embeddings\n",
    "# data = torch.load('/home/ldy/Workspace/THINGS/CLIP/ViT-H-14_features_train.pt', map_location='cuda:3')\n",
    "# emb_img_train = data['img_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "\n",
    "# torch.save(emb_img_test.cpu().detach(), 'variables/ViT-H-14_features_test.pt')\n",
    "# torch.save(emb_img_train.cpu().detach(), 'variables/ViT-H-14_features_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_test.shape, emb_img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1654clsx10imgsx4trials=66160\n",
    "emb_eeg = torch.load('/home/ldy/Workspace/Generation/masked_ATM_S_eeg_features_0_800_sub-08_train.pt')\n",
    "\n",
    "emb_eeg_test = torch.load('/home/ldy/Workspace/Generation/masked_ATM_S_eeg_features_0_800_sub-08_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eeg.shape, emb_eeg_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training prior diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, c_embeddings=None, h_embeddings=None, h_embeds_uncond=None, cond_sampling_rate=0.5):\n",
    "        self.c_embeddings = c_embeddings\n",
    "        self.h_embeddings = h_embeddings\n",
    "        self.N_cond = 0 if self.h_embeddings is None else len(self.h_embeddings)\n",
    "        self.h_embeds_uncond = h_embeds_uncond\n",
    "        self.N_uncond = 0 if self.h_embeds_uncond is None else len(self.h_embeds_uncond)\n",
    "        self.cond_sampling_rate = cond_sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N_cond\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"c_embedding\": self.c_embeddings[idx],\n",
    "            \"h_embedding\": self.h_embeddings[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = '/mnt/dataset0/weichen/projects/visobj/proposals/mise/data'\n",
    "# image_features = torch.load(os.path.join(path_data, 'openclip_emb/emb_imgnet.pt')) # 'emb_imgnet' or 'image_features'\n",
    "# h_embeds_imgnet = image_features['image_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=emb_eeg, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_prior = DiffusionPrior(dropout=0.1)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(f'./ckpts/masked_{model_name}_time_oneprior.pt', map_location=device))\n",
    "# pipe.train(dataloader, num_epochs=150, learning_rate=1e-3) # to 0.142 \n",
    "# torch.save(pipe.diffusion_prior.state_dict(), f'./ckpts/masked_{model_name}_0_800.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "\n",
    "\n",
    "# path of ground truth: /home/ldy/Workspace/THINGS/images_set/test_images\n",
    "k = 170\n",
    "image_embeds = emb_img_test[k:k+1]\n",
    "print(\"image_embeds\", image_embeds.shape)\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed_value)\n",
    "image = generator.generate(image_embeds, generator=gen)\n",
    "display(image)\n",
    "\n",
    "\n",
    "gen.manual_seed(seed_value)\n",
    "image = generator.generate(emb_eeg_test[k:k+1], generator=gen)\n",
    "display(image)\n",
    "\n",
    "\n",
    "# k = 0\n",
    "gen.manual_seed(seed_value)\n",
    "eeg_embeds = emb_eeg_test[k:k+1]\n",
    "print(\"image_embeds\", eeg_embeds.shape)\n",
    "h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0,generator=gen)\n",
    "image = generator.generate(h.to(dtype=torch.float16), generator=gen)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Assuming `generator`, `pipe`, and `emb_eeg_test` are predefined and properly set up\n",
    "# `device` should also be defined (e.g., 'cuda' or 'cpu')\n",
    "\n",
    "def create_dirs(base_dir, subdirs):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    for subdir in subdirs:\n",
    "        dir_path = os.path.join(base_dir, str(subdir))\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "def generate_and_save_images(k_values, base_dir='THINGS', num_seeds=30):\n",
    "    for k in k_values:\n",
    "        k_dir = os.path.join(base_dir, str(k))\n",
    "        eeg_embeds = emb_eeg_test[k:k+1]  # Assuming emb_eeg_test is indexed by k\n",
    "        for seed in range(num_seeds):\n",
    "            gen = torch.Generator(device=device).manual_seed(seed)\n",
    "            h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0, generator=gen)\n",
    "            image = generator.generate(h.to(dtype=torch.float16), generator=gen)\n",
    "            \n",
    "            # Assuming `image` is a PIL Image or similar; if it's a tensor, you'll need to convert it\n",
    "            image_file = os.path.join(k_dir, f\"{seed}.png\")  # Save as PNG\n",
    "            image.save(image_file)\n",
    "\n",
    "# Create 'THIN GS' directory and subdirectories for given k values\n",
    "k_values = [10, 11, 20, 58, 177, 93, 141, 160, 100, 108, 111, 115]\n",
    "base_dir = 'THINGS/0-800'\n",
    "create_dirs(base_dir, k_values)\n",
    "\n",
    "# Generate and save images\n",
    "generate_and_save_images(k_values, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
