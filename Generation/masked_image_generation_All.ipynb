{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['http_proxy'] = 'http://10.16.35.10:13390' \n",
    "# os.environ['https_proxy'] = 'http://10.16.35.10:13390' \n",
    "# os.environ['PATH'] += os.pathsep + '/usr/local/texlive/2023/bin/x86_64-linux'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "os.environ['http_proxy'] = 'http://10.16.35.10:13390' \n",
    "os.environ['https_proxy'] = 'http://10.16.35.10:13390' \n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from eegdatasets_leaveone_topo import EEGDataset\n",
    "from eegencoder import eeg_encoder\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from lavis.models.clip_models.loss import ClipLoss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from utils import wandb_logger\n",
    "from torch import Tensor\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model + 1, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[:d_model // 2 + 1])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[:d_model // 2])\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:x.size(0), :].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EEGAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(EEGAttention, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(2, 0, 1)  # Change shape to [time_length, batch_size, channel]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output.permute(1, 2, 0)  # Change shape back to [batch_size, channel, time_length]\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "        # revised from shallownet\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), (1, 1)),\n",
    "            nn.AvgPool2d((1, 51), (1, 5)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(40, 40, (63, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),  \n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b, _, _, _ = x.shape\n",
    "        x = x.unsqueeze(1)     \n",
    "        # print(\"x\", x.shape)   \n",
    "        x = self.tsconv(x)\n",
    "        # print(\"tsconv\", x.shape)   \n",
    "        x = self.projection(x)\n",
    "        # print(\"projection\", x.shape)  \n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(emb_size),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "        \n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1440, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class Proj_img(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1024, proj_dim=1024, drop_proj=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x \n",
    "\n",
    "\n",
    "class Padding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Padding, self).__init__()\n",
    "        self.region_to_electrodes = {\n",
    "            'Frontal': ['Fp1', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3', 'F1', 'F2', 'F4', 'F6', 'F8'],\n",
    "            'Central': ['FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6'],\n",
    "            'Parietal': ['CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8'],\n",
    "            'Temporal': ['FT9', 'FT7', 'T7', 'TP9', 'TP7', 'TP8', 'TP10', 'T8', 'FT8', 'FT10'],\n",
    "            'Occipital': ['PO7', 'PO3', 'POz', 'PO4', 'PO8', 'O1', 'Oz', 'O2'],\n",
    "            # 'Whole_brain': ['Fp1', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3', 'F1', 'F2', 'F4', 'F6', 'F8',\n",
    "            #                 'FC5', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4', 'FC6', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4', 'C6',\n",
    "            #                 'FT9', 'FT7', 'T7', 'TP9', 'TP7', 'TP8', 'TP10', 'T8', 'FT8', 'FT10',\n",
    "            #                 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8',\n",
    "            #                 'PO7', 'PO3', 'POz', 'PO4', 'PO8', 'O1', 'Oz', 'O2']\n",
    "            'Whole_brain': ['Fp1', 'Fp2', 'AF7', 'AF3', 'AFz', 'AF4', 'AF8', 'F7', 'F5', 'F3',\n",
    "\t\t\t\t  'F1', 'F2', 'F4', 'F6', 'F8', 'FT9', 'FT7', 'FC5', 'FC3', 'FC1', \n",
    "\t\t\t\t  'FCz', 'FC2', 'FC4', 'FC6', 'FT8', 'FT10', 'T7', 'C5', 'C3', 'C1',\n",
    "\t\t\t\t  'Cz', 'C2', 'C4', 'C6', 'T8', 'TP9', 'TP7', 'CP5', 'CP3', 'CP1', \n",
    "\t\t\t\t  'CPz', 'CP2', 'CP4', 'CP6', 'TP8', 'TP10', 'P7', 'P5', 'P3', 'P1',\n",
    "\t\t\t\t  'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7', 'PO3', 'POz', 'PO4', 'PO8',\n",
    "\t\t\t\t  'O1', 'Oz', 'O2']\n",
    "        }\n",
    "\n",
    "    def forward(self, x, region_name):\n",
    "        num_total_electrodes = len(self.region_to_electrodes['Whole_brain'])\n",
    "        x_padded = torch.zeros(x.size(0), num_total_electrodes, x.size(2), device=x.device)\n",
    "        current_region_electrodes = self.region_to_electrodes.get(region_name, [])\n",
    "        electrode_indices = [self.region_to_electrodes['Whole_brain'].index(e) for e in current_region_electrodes]\n",
    "        for idx, electrode_idx in enumerate(electrode_indices):\n",
    "            x_padded[:, electrode_idx, :] = x[:, idx, :]\n",
    "        return x_padded\n",
    "\n",
    "\n",
    "class ATMS(nn.Module):    \n",
    "    def __init__(self, num_channels=63, sequence_length=250, num_subjects=1, num_features=64, num_latents=1024, num_blocks=1):\n",
    "        super(ATMS, self).__init__()\n",
    "        self.attention_model = EEGAttention(num_channels, nhead=1)   \n",
    "        self.subject_wise_linear = nn.ModuleList([nn.Linear(sequence_length, sequence_length) for _ in range(num_subjects)])\n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg()        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_func = ClipLoss()       \n",
    "        self.padding = Padding()\n",
    "\n",
    "    def forward(self, x, region_name):\n",
    "        x = self.padding(x, region_name)\n",
    "\n",
    "        x = self.attention_model(x)\n",
    "        x = self.subject_wise_linear[0](x)\n",
    "        eeg_embedding = self.enc_eeg(x)\n",
    "        \n",
    "        out = self.proj_eeg(eeg_embedding)\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_eegfeatures(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k, mode):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.9\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    ridge_lambda = 0.1\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            eeg_data = eeg_data.to(device)            \n",
    "            \n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "            eeg_features = eegmodel(eeg_data, 'Whole_brain').float()\n",
    "            \n",
    "            features_list.append(eeg_features)\n",
    "            logit_scale = eegmodel.logit_scale \n",
    "                   \n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"eeg_features\", eeg_features.shape)\n",
    "            # print(torch.std(eeg_features, dim=-1))\n",
    "            # print(torch.std(img_features, dim=-1))\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)            \n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)       \n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "                \n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1        \n",
    "                total += 1\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            # print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"masked_ATM_S_eeg_features_Whole_brain_{sub}_{mode}.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()\n",
    "\n",
    "\n",
    "from IPython.display import Image, display\n",
    "config = {\n",
    "\"data_path\": \"/home/ldy/Workspace/THINGS/Preprocessed_data_250Hz\",\n",
    "\"project\": \"train_pos_img_text_rep\",\n",
    "\"entity\": \"sustech_rethinkingbci\",\n",
    "\"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "\"lr\": 3e-4,\n",
    "\"epochs\": 50,\n",
    "\"batch_size\": 1024,\n",
    "\"logger\": True,\n",
    "\"encoder_type\":'ATM_S_reconstruction_scale_0_1000',\n",
    "}\n",
    "\n",
    " \n",
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "data_path = config['data_path']\n",
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "eeg_model = ATMS(63, 250)\n",
    "print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()]))\n",
    "\n",
    "#####################################################################################\n",
    "# eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/Reconstruction/models/contrast/sub-08/01-30_00-44/40.pth\"))\n",
    "eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/BrainAligning_retrieval/models/masked_areas/ATMS/sub-08/02-21_13-19/40.pth\"))\n",
    "eeg_model.to(device)\n",
    "sub = 'sub-08'\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "test_dataset = EEGDataset(\"all\", data_path, subjects= [sub], train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = test_dataset.text_features\n",
    "img_features_test_all = test_dataset.img_features\n",
    "test_loss, test_accuracy,labels, eeg_features_test = get_eegfeatures(sub, eeg_model, test_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"test\")\n",
    "print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################################\n",
    "# train_dataset = EEGDataset(\"all\", data_path, subjects= [sub], train=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "# text_features_test_all = train_dataset.text_features\n",
    "# img_features_test_all = train_dataset.img_features\n",
    "\n",
    "# train_loss, train_accuracy, labels, eeg_features_train = get_eegfeatures(sub, eeg_model, train_loader, device, text_features_test_all, img_features_test_all,k=200, mode=\"train\")\n",
    "# print(f\" - Test Loss: {train_loss:.4f}, Test Accuracy: {train_accuracy:.4f}\")\n",
    "# #####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import sys\n",
    "from diffusion_prior import *\n",
    "from custom_pipeline import *\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" \n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load eeg and image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image embeddings\n",
    "# data = torch.load('/home/ldy/Workspace/THINGS/CLIP/ViT-H-14_features_test.pt', map_location='cuda:3')\n",
    "# emb_img_test = data['img_features']\n",
    "\n",
    "# # load image embeddings\n",
    "# data = torch.load('/home/ldy/Workspace/THINGS/CLIP/ViT-H-14_features_train.pt', map_location='cuda:3')\n",
    "# emb_img_train = data['img_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "\n",
    "# torch.save(emb_img_test.cpu().detach(), 'variables/ViT-H-14_features_test.pt')\n",
    "# torch.save(emb_img_train.cpu().detach(), 'variables/ViT-H-14_features_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_test.shape, emb_img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1654clsx10imgsx4trials=66160\n",
    "emb_eeg = torch.load('/home/ldy/Workspace/Generation/masked_ATM_S_eeg_features_Whole_brain_sub-08_train.pt')\n",
    "\n",
    "emb_eeg_test = torch.load('/home/ldy/Workspace/Generation/masked_ATM_S_eeg_features_Whole_brain_sub-08_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eeg.shape, emb_eeg_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training prior diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "\n",
    "    def __init__(self, c_embeddings=None, h_embeddings=None, h_embeds_uncond=None, cond_sampling_rate=0.5):\n",
    "        self.c_embeddings = c_embeddings\n",
    "        self.h_embeddings = h_embeddings\n",
    "        self.N_cond = 0 if self.h_embeddings is None else len(self.h_embeddings)\n",
    "        self.h_embeds_uncond = h_embeds_uncond\n",
    "        self.N_uncond = 0 if self.h_embeds_uncond is None else len(self.h_embeds_uncond)\n",
    "        self.cond_sampling_rate = cond_sampling_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N_cond\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"c_embedding\": self.c_embeddings[idx],\n",
    "            \"h_embedding\": self.h_embeddings[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_img_train_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = '/mnt/dataset0/weichen/projects/visobj/proposals/mise/data'\n",
    "# image_features = torch.load(os.path.join(path_data, 'openclip_emb/emb_imgnet.pt')) # 'emb_imgnet' or 'image_features'\n",
    "# h_embeds_imgnet = image_features['image_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=emb_eeg, h_embeddings=emb_img_train_4, \n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_prior = DiffusionPrior(dropout=0.1)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(f'./ckpts/masked_{model_name}_Whole_brain.pt', map_location=device))\n",
    "# pipe.train(dataloader, num_epochs=150, learning_rate=1e-3) # to 0.142 \n",
    "# torch.save(pipe.diffusion_prior.state_dict(), f'./ckpts/masked_{model_name}_Whole_brain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# from IPython.display import Image, display\n",
    "# generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "\n",
    "\n",
    "# path of ground truth: /home/ldy/Workspace/THINGS/images_set/test_images\n",
    "k = 105\n",
    "image_embeds = emb_img_test[k:k+1]\n",
    "print(\"image_embeds\", image_embeds.shape)\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed_value)\n",
    "image = generator.generate(image_embeds, generator=gen)\n",
    "display(image)\n",
    "\n",
    "\n",
    "gen.manual_seed(seed_value)\n",
    "image = generator.generate(emb_eeg_test[k:k+1], generator=gen)\n",
    "display(image)\n",
    "\n",
    "\n",
    "# k = 0\n",
    "gen.manual_seed(seed_value)\n",
    "eeg_embeds = emb_eeg_test[k:k+1]\n",
    "print(\"image_embeds\", eeg_embeds.shape)\n",
    "h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0,generator=gen)\n",
    "image = generator.generate(h.to(dtype=torch.float16), generator=gen)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
